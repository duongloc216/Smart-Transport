{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5b34d5",
   "metadata": {},
   "source": [
    "# ðŸš— Smart Traffic System - Model Training on Google Colab\n",
    "\n",
    "**Goal:** Train XGBoost + LightGBM + Prophet models on Google Colab (Free T4 GPU)\n",
    "\n",
    "**Steps:**\n",
    "1. Mount Google Drive\n",
    "2. Load data from CSV\n",
    "3. Feature engineering\n",
    "4. Train 3 models\n",
    "5. Save models to Drive\n",
    "6. Download to local machine\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ba66a",
   "metadata": {},
   "source": [
    "## ðŸ“¦ STEP 1: Setup & Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q xgboost lightgbm prophet scikit-learn pandas numpy matplotlib seaborn plotly joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff759a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "import joblib\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b8604",
   "metadata": {},
   "source": [
    "## ðŸ“ STEP 2: Mount Google Drive & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to your data\n",
    "DATA_PATH = '/content/drive/MyDrive/SmartTraffic/data/traffic_data_for_training.csv'\n",
    "MODEL_OUTPUT_PATH = '/content/drive/MyDrive/SmartTraffic/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe72311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"ðŸ“Š Loading traffic data...\")\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['DateObservedFrom', 'DateObservedTo'])\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} records\")\n",
    "print(f\"\\nðŸ“‹ Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nðŸ“ˆ Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b48315",
   "metadata": {},
   "source": [
    "## ðŸ”§ STEP 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a588dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create 18+ features for ML models\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Time features\n",
    "    df['hour'] = df['DateObservedFrom'].dt.hour\n",
    "    df['day_of_week'] = df['DateObservedFrom'].dt.dayofweek  # 0=Monday\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) | \n",
    "                           (df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
    "    \n",
    "    # 2. Sort by segment and time\n",
    "    df = df.sort_values(['RefRoadSegment', 'DateObservedFrom']).reset_index(drop=True)\n",
    "    \n",
    "    # 3. Lag features (previous values)\n",
    "    df['speed_lag_1'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(1)\n",
    "    df['speed_lag_2'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(2)\n",
    "    df['speed_lag_3'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(3)\n",
    "    \n",
    "    df['intensity_lag_1'] = df.groupby('RefRoadSegment')['Intensity'].shift(1)\n",
    "    \n",
    "    # 4. Rolling statistics\n",
    "    df['speed_rolling_mean_6'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(6, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['speed_rolling_mean_12'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(12, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['speed_rolling_std_6'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(6, min_periods=1).std().reset_index(0, drop=True)\n",
    "    \n",
    "    df['intensity_rolling_mean_6'] = df.groupby('RefRoadSegment')['Intensity'].rolling(6, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # 5. Difference features\n",
    "    df['speed_diff'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].diff()\n",
    "    df['intensity_diff'] = df.groupby('RefRoadSegment')['Intensity'].diff()\n",
    "    \n",
    "    # 6. Ratio features\n",
    "    df['speed_to_max_ratio'] = df['AverageVehicleSpeed'] / df['MaximumAllowedSpeed']\n",
    "    \n",
    "    # 7. Segment encoding (one-hot)\n",
    "    segment_dummies = pd.get_dummies(df['RefRoadSegment'], prefix='segment')\n",
    "    df = pd.concat([df, segment_dummies], axis=1)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    print(f\"âœ… Created features. New shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_featured = create_features(df)\n",
    "print(\"\\nðŸ“Š Sample features:\")\n",
    "df_featured[['AverageVehicleSpeed', 'hour', 'is_rush_hour', 'speed_lag_1', 'speed_rolling_mean_6']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060b6a4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ STEP 4: Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns for ML\n",
    "feature_cols = [\n",
    "    'Intensity', 'Occupancy', 'TotalLaneNumber', 'MaximumAllowedSpeed',\n",
    "    'hour', 'day_of_week', 'is_weekend', 'is_rush_hour',\n",
    "    'speed_lag_1', 'speed_lag_2', 'speed_lag_3',\n",
    "    'intensity_lag_1',\n",
    "    'speed_rolling_mean_6', 'speed_rolling_mean_12', 'speed_rolling_std_6',\n",
    "    'intensity_rolling_mean_6',\n",
    "    'speed_diff', 'intensity_diff',\n",
    "    'speed_to_max_ratio'\n",
    "] + [col for col in df_featured.columns if col.startswith('segment_')]\n",
    "\n",
    "# Targets\n",
    "target_speed = 'AverageVehicleSpeed'\n",
    "target_congestion = 'Congested'\n",
    "\n",
    "# Create X and y\n",
    "X = df_featured[feature_cols]\n",
    "y_speed = df_featured[target_speed]\n",
    "y_congestion = df_featured[target_congestion].astype(int)\n",
    "\n",
    "# Train/Test split (80/20)\n",
    "X_train, X_test, y_speed_train, y_speed_test, y_cong_train, y_cong_test = train_test_split(\n",
    "    X, y_speed, y_congestion, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train set: {len(X_train)} records\")\n",
    "print(f\"âœ… Test set: {len(X_test)} records\")\n",
    "print(f\"\\nðŸ“Š Features: {len(feature_cols)}\")\n",
    "print(f\"\\nðŸ“‹ Congestion distribution:\")\n",
    "print(f\"  Train - Flowing: {(y_cong_train==0).sum()} | Congested: {(y_cong_train==1).sum()}\")\n",
    "print(f\"  Test  - Flowing: {(y_cong_test==0).sum()} | Congested: {(y_cong_test==1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ… Features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1cedb9",
   "metadata": {},
   "source": [
    "## ðŸš€ STEP 5A: Train XGBoost (Congestion Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c568672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING XGBOOST CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = (y_cong_train == 0).sum() / (y_cong_train == 1).sum()\n",
    "print(f\"\\nâš–ï¸  Class imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'  # Fast training\n",
    ")\n",
    "\n",
    "# Train\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_cong_train,\n",
    "    eval_set=[(X_test_scaled, y_cong_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š XGBOOST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â±ï¸  Training time: {train_time:.1f} seconds\")\n",
    "print(f\"\\n{classification_report(y_cong_test, y_pred, target_names=['Flowing', 'Congested'])}\")\n",
    "\n",
    "# Feature importance (Top 15)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Top 15 Important Features:\")\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance (Top 15)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99626d3d",
   "metadata": {},
   "source": [
    "## ðŸš€ STEP 5B: Train LightGBM (Speed Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0448d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING LIGHTGBM REGRESSOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_samples=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "lgb_model.fit(\n",
    "    X_train_scaled, y_speed_train,\n",
    "    eval_set=[(X_test_scaled, y_speed_test)],\n",
    "    callbacks=[lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_speed_pred = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_speed_test, y_speed_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_speed_test, y_speed_pred))\n",
    "r2 = r2_score(y_speed_test, y_speed_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š LIGHTGBM RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â±ï¸  Training time: {train_time:.1f} seconds\")\n",
    "print(f\"\\nðŸ“ˆ Metrics:\")\n",
    "print(f\"  MAE:  {mae:.2f} km/h\")\n",
    "print(f\"  RMSE: {rmse:.2f} km/h\")\n",
    "print(f\"  RÂ²:   {r2:.4f}\")\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_speed_test, y_speed_pred, alpha=0.3)\n",
    "plt.plot([y_speed_test.min(), y_speed_test.max()], \n",
    "         [y_speed_test.min(), y_speed_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Speed (km/h)')\n",
    "plt.ylabel('Predicted Speed (km/h)')\n",
    "plt.title(f'LightGBM: Actual vs Predicted (MAE={mae:.2f}, RÂ²={r2:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb50f1da",
   "metadata": {},
   "source": [
    "## ðŸš€ STEP 5C: Train Prophet (Trend per Segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING PROPHET MODELS (Per Segment)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare Prophet data\n",
    "prophet_models = {}\n",
    "segments = df_featured['RefRoadSegment'].unique()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"\\nðŸ“Š Training Prophet for {segment}...\")\n",
    "    \n",
    "    # Filter segment data\n",
    "    df_segment = df_featured[df_featured['RefRoadSegment'] == segment].copy()\n",
    "    \n",
    "    # Prophet format\n",
    "    df_prophet = pd.DataFrame({\n",
    "        'ds': df_segment['DateObservedFrom'],\n",
    "        'y': df_segment['AverageVehicleSpeed'],\n",
    "        'intensity': df_segment['Intensity'],\n",
    "        'is_weekend': df_segment['is_weekend'],\n",
    "        'is_rush_hour': df_segment['is_rush_hour']\n",
    "    })\n",
    "    \n",
    "    # Initialize Prophet\n",
    "    model = Prophet(\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=False,\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=10\n",
    "    )\n",
    "    \n",
    "    # Add regressors\n",
    "    model.add_regressor('intensity')\n",
    "    model.add_regressor('is_weekend')\n",
    "    model.add_regressor('is_rush_hour')\n",
    "    \n",
    "    # Train\n",
    "    model.fit(df_prophet)\n",
    "    \n",
    "    prophet_models[segment] = model\n",
    "    print(f\"  âœ… {segment} trained\")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š PROPHET RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"â±ï¸  Total training time: {train_time:.1f} seconds\")\n",
    "print(f\"âœ… Trained {len(prophet_models)} models (one per segment)\")\n",
    "\n",
    "# Sample forecast visualization for one segment\n",
    "sample_segment = segments[0]\n",
    "sample_model = prophet_models[sample_segment]\n",
    "\n",
    "# Forecast next 24 hours\n",
    "future = sample_model.make_future_dataframe(periods=288, freq='5min')  # 288 = 24h / 5min\n",
    "# Add regressors (use mean values for future)\n",
    "future['intensity'] = df_featured[df_featured['RefRoadSegment']==sample_segment]['Intensity'].mean()\n",
    "future['is_weekend'] = 0\n",
    "future['is_rush_hour'] = 0\n",
    "\n",
    "forecast = sample_model.predict(future)\n",
    "\n",
    "# Plot\n",
    "fig = sample_model.plot(forecast)\n",
    "plt.title(f'Prophet Forecast - {sample_segment}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Components plot\n",
    "fig = sample_model.plot_components(forecast)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d0de3",
   "metadata": {},
   "source": [
    "## ðŸ’¾ STEP 6: Save Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING TRAINED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save XGBoost\n",
    "xgb_path = os.path.join(MODEL_OUTPUT_PATH, 'xgboost_congestion.pkl')\n",
    "joblib.dump(xgb_model, xgb_path)\n",
    "print(f\"âœ… XGBoost saved: {xgb_path}\")\n",
    "print(f\"   Size: {os.path.getsize(xgb_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save LightGBM\n",
    "lgb_path = os.path.join(MODEL_OUTPUT_PATH, 'lightgbm_speed.pkl')\n",
    "joblib.dump(lgb_model, lgb_path)\n",
    "print(f\"âœ… LightGBM saved: {lgb_path}\")\n",
    "print(f\"   Size: {os.path.getsize(lgb_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save Prophet (all segments)\n",
    "prophet_path = os.path.join(MODEL_OUTPUT_PATH, 'prophet_models.pkl')\n",
    "joblib.dump(prophet_models, prophet_path)\n",
    "print(f\"âœ… Prophet saved: {prophet_path}\")\n",
    "print(f\"   Size: {os.path.getsize(prophet_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save Scaler\n",
    "scaler_path = os.path.join(MODEL_OUTPUT_PATH, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler saved: {scaler_path}\")\n",
    "print(f\"   Size: {os.path.getsize(scaler_path) / 1024:.1f} KB\")\n",
    "\n",
    "# Save feature columns\n",
    "feature_cols_path = os.path.join(MODEL_OUTPUT_PATH, 'feature_columns.pkl')\n",
    "joblib.dump(feature_cols, feature_cols_path)\n",
    "print(f\"âœ… Feature columns saved: {feature_cols_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ‰ ALL MODELS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nðŸ“‹ NEXT STEPS:\")\n",
    "print(\"  1. Download models from Google Drive to local:\")\n",
    "print(f\"     {MODEL_OUTPUT_PATH}\")\n",
    "print(\"  2. Place in: e:\\\\CÄTT2\\\\Smart-Transport\\\\...\\\\models\\\\saved_models\\\\\")\n",
    "print(\"  3. Load models in FastAPI for serving\")\n",
    "print(\"  4. Test predictions locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d582178",
   "metadata": {},
   "source": [
    "## ðŸ“Š STEP 7: Model Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š FINAL MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'XGBoost Classifier',\n",
    "        'Task': 'Congestion Detection',\n",
    "        'Metric': f\"{f1_score(y_cong_test, y_pred):.3f} F1-Score\",\n",
    "        'File': 'xgboost_congestion.pkl'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'LightGBM Regressor',\n",
    "        'Task': 'Speed Prediction',\n",
    "        'Metric': f\"{mae:.2f} km/h MAE\",\n",
    "        'File': 'lightgbm_speed.pkl'\n",
    "    },\n",
    "    {\n",
    "        'Model': f'Prophet ({len(prophet_models)} models)',\n",
    "        'Task': 'Trend Forecasting',\n",
    "        'Metric': 'Per-segment trend',\n",
    "        'File': 'prophet_models.pkl'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "print(\"\\nâœ… Training completed successfully!\")\n",
    "print(\"ðŸ“ All models saved to Google Drive\")\n",
    "print(\"ðŸš€ Ready for deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d35b29",
   "metadata": {},
   "source": [
    "# ðŸš— Smart Traffic System - Model Training\n",
    "\n",
    "**Train XGBoost + LightGBM + Prophet on Google Colab (Free T4 GPU)**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Steps:\n",
    "1. Mount Google Drive\n",
    "2. Load data (8,650 records)\n",
    "3. Feature engineering (18+ features)\n",
    "4. Train 3 models (~20 minutes)\n",
    "5. Save models to Drive\n",
    "6. Download to local machine\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231853b",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56917dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost lightgbm prophet scikit-learn pandas numpy matplotlib seaborn plotly joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "print(f\"XGBoost: {xgb.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdd775",
   "metadata": {},
   "source": [
    "## ðŸ“ Step 2: Mount Google Drive & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dec6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATA_PATH = '/content/drive/MyDrive/SmartTraffic/data/traffic_data_for_training.csv'\n",
    "MODEL_OUTPUT_PATH = '/content/drive/MyDrive/SmartTraffic/models/'\n",
    "\n",
    "print(\"âœ… Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Loading data...\")\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['DateObservedFrom', 'DateObservedTo'])\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} records\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8631f18",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca09cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time features\n",
    "    df['hour'] = df['DateObservedFrom'].dt.hour\n",
    "    df['day_of_week'] = df['DateObservedFrom'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_rush_hour'] = ((df['hour'] >= 7) & (df['hour'] <= 9) | \n",
    "                           (df['hour'] >= 17) & (df['hour'] <= 19)).astype(int)\n",
    "    \n",
    "    # Sort\n",
    "    df = df.sort_values(['RefRoadSegment', 'DateObservedFrom']).reset_index(drop=True)\n",
    "    \n",
    "    # Lag features\n",
    "    df['speed_lag_1'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(1)\n",
    "    df['speed_lag_2'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(2)\n",
    "    df['speed_lag_3'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].shift(3)\n",
    "    df['intensity_lag_1'] = df.groupby('RefRoadSegment')['Intensity'].shift(1)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df['speed_rolling_mean_6'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(6, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['speed_rolling_mean_12'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(12, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    df['speed_rolling_std_6'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].rolling(6, min_periods=1).std().reset_index(0, drop=True)\n",
    "    df['intensity_rolling_mean_6'] = df.groupby('RefRoadSegment')['Intensity'].rolling(6, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Difference features\n",
    "    df['speed_diff'] = df.groupby('RefRoadSegment')['AverageVehicleSpeed'].diff()\n",
    "    df['intensity_diff'] = df.groupby('RefRoadSegment')['Intensity'].diff()\n",
    "    \n",
    "    # Ratio\n",
    "    df['speed_to_max_ratio'] = df['AverageVehicleSpeed'] / df['MaximumAllowedSpeed']\n",
    "    \n",
    "    # One-hot encoding\n",
    "    segment_dummies = pd.get_dummies(df['RefRoadSegment'], prefix='segment')\n",
    "    df = pd.concat([df, segment_dummies], axis=1)\n",
    "    \n",
    "    # Fill NaN\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_featured = create_features(df)\n",
    "print(f\"âœ… Features created. Shape: {df_featured.shape}\")\n",
    "df_featured[['AverageVehicleSpeed', 'hour', 'is_rush_hour', 'speed_lag_1', 'speed_rolling_mean_6']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6a2ce",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 4: Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'Intensity', 'Occupancy', 'TotalLaneNumber', 'MaximumAllowedSpeed',\n",
    "    'hour', 'day_of_week', 'is_weekend', 'is_rush_hour',\n",
    "    'speed_lag_1', 'speed_lag_2', 'speed_lag_3', 'intensity_lag_1',\n",
    "    'speed_rolling_mean_6', 'speed_rolling_mean_12', 'speed_rolling_std_6',\n",
    "    'intensity_rolling_mean_6', 'speed_diff', 'intensity_diff', 'speed_to_max_ratio'\n",
    "] + [col for col in df_featured.columns if col.startswith('segment_')]\n",
    "\n",
    "X = df_featured[feature_cols]\n",
    "y_speed = df_featured['AverageVehicleSpeed']\n",
    "y_congestion = df_featured['Congested'].astype(int)\n",
    "\n",
    "X_train, X_test, y_speed_train, y_speed_test, y_cong_train, y_cong_test = train_test_split(\n",
    "    X, y_speed, y_congestion, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"\\nCongestion - Train: Flowing={(y_cong_train==0).sum()}, Congested={(y_cong_train==1).sum()}\")\n",
    "print(f\"Congestion - Test:  Flowing={(y_cong_test==0).sum()}, Congested={(y_cong_test==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1313dab",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 5A: Train XGBoost (Congestion Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start = time.time()\n",
    "scale_pos_weight = (y_cong_train == 0).sum() / (y_cong_train == 1).sum()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42, n_jobs=-1, tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_cong_train, eval_set=[(X_test_scaled, y_cong_test)], verbose=50)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_scaled)\n",
    "train_time = time.time() - start\n",
    "\n",
    "print(f\"\\nâ±ï¸  Training: {train_time:.1f}s\")\n",
    "print(f\"\\n{classification_report(y_cong_test, y_pred, target_names=['Flowing', 'Congested'])}\")\n",
    "\n",
    "# Feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "print(\"\\nTop 15 Features:\")\n",
    "print(feat_imp.to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_imp['feature'], feat_imp['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d110531",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 5B: Train LightGBM (Speed Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING LIGHTGBM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train_scaled, y_speed_train, eval_set=[(X_test_scaled, y_speed_test)], callbacks=[lgb.log_evaluation(50)])\n",
    "\n",
    "y_speed_pred = lgb_model.predict(X_test_scaled)\n",
    "train_time = time.time() - start\n",
    "\n",
    "mae = mean_absolute_error(y_speed_test, y_speed_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_speed_test, y_speed_pred))\n",
    "r2 = r2_score(y_speed_test, y_speed_pred)\n",
    "\n",
    "print(f\"\\nâ±ï¸  Training: {train_time:.1f}s\")\n",
    "print(f\"\\nMAE:  {mae:.2f} km/h\")\n",
    "print(f\"RMSE: {rmse:.2f} km/h\")\n",
    "print(f\"RÂ²:   {r2:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_speed_test, y_speed_pred, alpha=0.3)\n",
    "plt.plot([y_speed_test.min(), y_speed_test.max()], [y_speed_test.min(), y_speed_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Speed (km/h)')\n",
    "plt.ylabel('Predicted Speed (km/h)')\n",
    "plt.title(f'LightGBM: MAE={mae:.2f}, RÂ²={r2:.3f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1878d",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 5C: Train Prophet (Per Segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad801a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¥ TRAINING PROPHET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "prophet_models = {}\n",
    "segments = df_featured['RefRoadSegment'].unique()\n",
    "start = time.time()\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"\\nðŸ“Š {segment}...\")\n",
    "    df_seg = df_featured[df_featured['RefRoadSegment'] == segment].copy()\n",
    "    \n",
    "    df_prophet = pd.DataFrame({\n",
    "        'ds': df_seg['DateObservedFrom'],\n",
    "        'y': df_seg['AverageVehicleSpeed'],\n",
    "        'intensity': df_seg['Intensity'],\n",
    "        'is_weekend': df_seg['is_weekend'],\n",
    "        'is_rush_hour': df_seg['is_rush_hour']\n",
    "    })\n",
    "    \n",
    "    model = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=False)\n",
    "    model.add_regressor('intensity')\n",
    "    model.add_regressor('is_weekend')\n",
    "    model.add_regressor('is_rush_hour')\n",
    "    model.fit(df_prophet)\n",
    "    \n",
    "    prophet_models[segment] = model\n",
    "    print(f\"  âœ… Done\")\n",
    "\n",
    "train_time = time.time() - start\n",
    "print(f\"\\nâ±ï¸  Total: {train_time:.1f}s\")\n",
    "print(f\"âœ… {len(prophet_models)} models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14288530",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 6: Save Models to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ’¾ SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_path = os.path.join(MODEL_OUTPUT_PATH, 'xgboost_congestion.pkl')\n",
    "joblib.dump(xgb_model, xgb_path)\n",
    "print(f\"âœ… XGBoost: {os.path.getsize(xgb_path)/1024:.1f} KB\")\n",
    "\n",
    "lgb_path = os.path.join(MODEL_OUTPUT_PATH, 'lightgbm_speed.pkl')\n",
    "joblib.dump(lgb_model, lgb_path)\n",
    "print(f\"âœ… LightGBM: {os.path.getsize(lgb_path)/1024:.1f} KB\")\n",
    "\n",
    "prophet_path = os.path.join(MODEL_OUTPUT_PATH, 'prophet_models.pkl')\n",
    "joblib.dump(prophet_models, prophet_path)\n",
    "print(f\"âœ… Prophet: {os.path.getsize(prophet_path)/1024:.1f} KB\")\n",
    "\n",
    "scaler_path = os.path.join(MODEL_OUTPUT_PATH, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"âœ… Scaler: {os.path.getsize(scaler_path)/1024:.1f} KB\")\n",
    "\n",
    "feat_path = os.path.join(MODEL_OUTPUT_PATH, 'feature_columns.pkl')\n",
    "joblib.dump(feature_cols, feat_path)\n",
    "print(f\"âœ… Features: {os.path.getsize(feat_path)/1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ ALL SAVED TO GOOGLE DRIVE!\")\n",
    "print(f\"Location: {MODEL_OUTPUT_PATH}\")\n",
    "print(\"\\nNext: Download to local machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7eefe2",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 7: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ðŸ“Š TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {'Model': 'XGBoost', 'Task': 'Congestion', 'Metric': f\"{f1_score(y_cong_test, y_pred):.3f} F1\", 'File': 'xgboost_congestion.pkl'},\n",
    "    {'Model': 'LightGBM', 'Task': 'Speed', 'Metric': f\"{mae:.2f} km/h MAE\", 'File': 'lightgbm_speed.pkl'},\n",
    "    {'Model': f'Prophet x{len(prophet_models)}', 'Task': 'Trend', 'Metric': 'Per-segment', 'File': 'prophet_models.pkl'}\n",
    "])\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(\"ðŸ“ Models saved to Google Drive\")\n",
    "print(\"ðŸš€ Ready for deployment\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
